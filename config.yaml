# Docling-Graph Configuration File
# This file defines default settings and model configurations

# Default settings (can be overridden via CLI flags)
defaults:
  processing_mode: many-to-one  # one-to-one | many-to-one
  backend_type: llm               # llm | vlm
  inference: local              # local | remote
  export_format: csv            # csv | cypher

# Docling pipeline configuration
docling:
  # Choose pipeline type for document conversion
  # - "ocr": Classic OCR pipeline (most accurate for standard documents)
  # - "vision": Vision-Language Model pipeline based on Granite-Docling (best for complex layouts)
  pipeline: ocr  # ocr | vision

# Model configurations organized by type and inference location
models:
  # Vision-Language Models (VLM)
  vlm:
    local:
      default_model: "numind/NuExtract-2.0-8B"
      provider: "docling"
      # Alternative models:
      # - "numind/NuExtract-2.0-2B" (faster, less accurate)
  
  # Language Models (LLM)
  llm:
    local:
      default_model: "llama3:8b-instruct"
      provider: "ollama"
      # Make sure Ollama is running locally with: ollama serve
      # Pull models with: ollama pull llama3:8b-instruct
      
    remote:
      default_model: "mistral-medium-latest"
      provider: "mistral"
      
      # Multiple API providers supported
      # Set API key: <PROVIDER>_API_KEY="your_key" using .env or export
      providers:
        mistral:
          default_model: "mistral-medium-latest"
          
        openai:
          default_model: "gpt-4-turbo"
          
        gemini:
          default_model: "gemini-2.5-flash"

# Output settings
output:
  default_directory: "outputs"
  create_visualizations: true
  create_markdown: true

# Docling-Graph Configuration File
# This file defines default settings and model configurations

# Default settings (can be overridden via CLI flags)
defaults:
  processing_mode: many-to-one  # one-to-one | many-to-one
  backend_type: llm             # llm | vlm
  inference: local              # local | api
  export_format: csv            # csv | cypher

# Docling pipeline configuration
docling:
  # Choose pipeline type for document conversion
  # - "ocr": Classic OCR pipeline (most accurate for standard documents)
  # - "vision": Vision-Language Model pipeline based on Granite-Docling (best for complex layouts)
  pipeline: default  # ocr | vision

# Model configurations organized by type and inference location
models:
  # Vision-Language Models (VLM)
  vlm:
    local:
      default_model: "numind/NuExtract-2.0-8B"
      provider: "docling"
      # Alternative models:
      # - "numind/NuExtract-2.0-2B" (faster, less accurate)
  
  # Language Models (LLM)
  llm:
    local:
      default_model: "ibm-granite/granite-4.0-1b"
      provider: "vllm"

      # Provider-specific configurations
      providers:
        vllm:
          default_model: "ibm-granite/granite-4.0-1b"
          base_url: "http://localhost:8000/v1"

        ollama:
          default_model: "llama-3.1-8b"
        
    remote:
      default_model: "mistral-small-latest"
      provider: "mistral"
      
      # Multiple API providers supported
      # Set API key: <PROVIDER>_API_KEY="your_key" using .env or export
      providers:
        mistral:
          default_model: "mistral-small-latest"
          
        openai:
          default_model: "gpt-4-turbo"
          
        gemini:
          default_model: "gemini-2.5-flash"

# Output settings
output:
  default_directory: "outputs"
  create_visualizations: true
  create_markdown: true
